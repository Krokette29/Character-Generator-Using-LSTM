{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of the character-level Elman RNN model.\n",
    "Written by Ngoc-Quan Pham based on Andreij Karparthy's lecture Cs231n.\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from random import uniform\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since numpy doesn't have a function for sigmoid\n",
    "# We implement it manually here\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# The derivative of the sigmoid function\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "# The derivative of the tanh function\n",
    "def dtanh(x):\n",
    "    return 1 - x*x\n",
    "\n",
    "\n",
    "# The numerically stable softmax implementation\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "\n",
    "\n",
    "# should be simple plain text file. I provided the sample from \"Hamlet - Shakespeares\"\n",
    "data = open('data/input.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters deciding the network size\n",
    "emb_size = 32  # word/character embedding size\n",
    "seq_length = 128  # number of steps to unroll the RNN for the truncated back-propagation algorithm\n",
    "hidden_size = 10\n",
    "# learning rate for the Adagrad algorithm. (this one is not 'optimized', only required to make the model learn)\n",
    "learning_rate = 1e-1\n",
    "std=0.1  # The standard deviation for parameter initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "# Here we initialize the parameters based an random uniform distribution, with the std of 0.01\n",
    "\n",
    "# word embedding: each character in the vocabulary is mapped to a vector with $emb_size$ neurons\n",
    "# Transform one-hot vectors to embedding X\n",
    "Wex = np.random.randn(emb_size, vocab_size) * std\n",
    "\n",
    "# weight to transform input X to hidden H\n",
    "Wxh = np.random.randn(hidden_size, emb_size) * std\n",
    "\n",
    "# weight to transform previous hidden states H_{t-1} to hidden H_t\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * std  # hidden to hidden\n",
    "\n",
    "# Output layer: transforming the hidden states H to output layer\n",
    "Why = np.random.randn(vocab_size, hidden_size) * std  # hidden to output\n",
    "\n",
    "# The biases are typically initialized as zeros. But sometimes people init them with uniform distribution too.\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "# These variables are momentums for the Adagrad algorithm\n",
    "# Each parameter in the network needs one momentum correspondingly\n",
    "mWex, mWxh, mWhh, mWhy = np.zeros_like(Wex), np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, labels, memory):\n",
    "\n",
    "    prev_h = memory\n",
    "    \"\"\"\n",
    "    # dictionaries to store the activations over time\n",
    "    # note from back-propagation implementation:\n",
    "    # back-propagation uses dynamic programming to estimate gradients efficiently\n",
    "    # so we need to store the activations over the course of the forward pass\n",
    "    # in the backward pass we will use the activations to compute the gradients\n",
    "    # (otherwise we will need to recompute them)\n",
    "    \"\"\"\n",
    "\n",
    "    # those variables stand for:\n",
    "    # xs: inputs to the RNNs at timesteps (embeddings)\n",
    "    # cs: characters at timesteps\n",
    "    # hs: hidden states at timesteps\n",
    "    # ys: output layers at timesteps\n",
    "    # ps: probability distributions at timesteps\n",
    "    xs, cs, hs, os, ps, ys = {}, {}, {}, {}, {}, {}\n",
    "\n",
    "    # the first memory (before training) is the previous (or initial) hidden state\n",
    "    hs[-1] = np.copy(prev_h)\n",
    "\n",
    "    # the loss will be accumulated over time\n",
    "    loss = 0\n",
    "\n",
    "    for t in range(len(inputs)):\n",
    "\n",
    "        # one-hot vector representation for character input at time t\n",
    "        cs[t] = np.zeros((vocab_size,1))\n",
    "        cs[t][inputs[t]] = 1\n",
    "\n",
    "        # transform the one hot vector to embedding\n",
    "        # x = Wemb x c\n",
    "        xs[t] = np.dot(Wex, cs[t])\n",
    "\n",
    "        # computation for the hidden state of the network\n",
    "        # H = tanh ( Wh . H + Wx . x )\n",
    "        h_pre_activation = np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + bh\n",
    "        hs[t] = np.tanh(h_pre_activation)\n",
    "\n",
    "        # output layer:\n",
    "        # this is the unnormalized log probabilities for next chars (across all chars in the vocabulary)\n",
    "        os[t] = np.dot(Why, hs[t]) + by\n",
    "\n",
    "        # softmax layer to get normalized probabilities:\n",
    "        ps[t] = softmax(os[t])\n",
    "\n",
    "        # cross entropy loss at time t:\n",
    "        ys[t] = np.zeros((vocab_size, 1))\n",
    "        ys[t][labels[t]] = 1\n",
    "\n",
    "        loss_t = np.sum(-np.log(ps[t]) * ys[t])\n",
    "\n",
    "        loss += loss_t\n",
    "\n",
    "    # packaging the activations to use in the backward pass\n",
    "    activations = (xs, cs, hs, os, ps, ys)\n",
    "    last_hidden = hs[-1]\n",
    "    return loss, activations, last_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(activations, clipping=True):\n",
    "    \"\"\"\n",
    "    during the backward pass we follow the track of the forward pass\n",
    "    the activations are needed so that we can avoid unnecessary re-computation\n",
    "    \"\"\"\n",
    "\n",
    "    # Gradient initialization\n",
    "    # Each parameter has a corresponding gradient (of the loss with respect to that gradient)\n",
    "    dWex, dWxh, dWhh, dWhy = np.zeros_like(Wex), np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "\n",
    "    xs, cs, hs, os, ps, ys = activations\n",
    "\n",
    "    # here we need the gradient w.r.t to the hidden layer at the final time step\n",
    "    # since this hidden layer is not connected to any future (final time step)\n",
    "    # then we can initialize it as zero vectors\n",
    "    dh = np.zeros_like(hs[0])\n",
    "\n",
    "    # the backward pass starts from the final step of the chain in the forward pass\n",
    "    for t in reversed(range(len(inputs))):\n",
    "\n",
    "        # first, we need to compute the gradients of the variable closest to the loss function,\n",
    "        # which is the softmax output p\n",
    "        # but here I skip it directly to the gradients of the unnormalized scores o because\n",
    "        # basically dL / do = p - y\n",
    "        # from the cross entropy gradients. (the explanation is a bit too long to write here)\n",
    "        do = ps[t] - ys[t]\n",
    "\n",
    "        # the gradients w.r.t to the weights and the bias that were used to create o[t]\n",
    "        dWhy += np.dot(do, hs[t].T)\n",
    "        dby += do\n",
    "\n",
    "        # because h is connected to both o and the next h, we sum the gradients up\n",
    "        dh = np.dot(Why.T, do) + dh\n",
    "\n",
    "        # backprop through the activation function (tanh)\n",
    "        dtanh_h = 1 - hs[t] * hs[t]\n",
    "        dh_pre_activation = dtanh_h * dh # because h = tanh(h_pre_activation)\n",
    "\n",
    "        # next, since  H = tanh ( Wh . H + Wx . x + bh )\n",
    "        # we use dh to backprop to dWh and dWx\n",
    "\n",
    "        # gradient of the bias and weight, this is similar to dby and dWhy\n",
    "        # for the H term\n",
    "        dbh += dh_pre_activation\n",
    "        dWhh += np.dot(dh_pre_activation, hs[t-1].T)\n",
    "        # we need this term for the recurrent connection (previous bptt step needs this)\n",
    "        dh = np.dot(Whh.T, dh_pre_activation)\n",
    "\n",
    "        # similarly for the x term\n",
    "        dWxh += np.dot(dh_pre_activation, xs[t].T)\n",
    "\n",
    "        # backward through the embedding\n",
    "        dx = np.dot(Wxh.T, dh_pre_activation)\n",
    "\n",
    "        # finally backward to the embedding projection\n",
    "        dWex += np.dot(dx, cs[t].T)\n",
    "\n",
    "    if clipping:\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "\n",
    "    gradients = (dWex, dWxh, dWhh, dWhy, dbh, dby)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    c = np.zeros((vocab_size, 1))\n",
    "    c[seed_ix] = 1\n",
    "    generated_chars = []\n",
    "    for t in range(n):\n",
    "        x = np.dot(Wex, c)\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        o = np.dot(Why, h) + by\n",
    "        p = softmax(o)\n",
    "\n",
    "        # the the distribution, we randomly generate samples:\n",
    "        ix = np.random.multinomial(1, p.ravel())\n",
    "        c = np.zeros((vocab_size, 1))\n",
    "\n",
    "        for j in range(len(ix)):\n",
    "            if ix[j] == 1:\n",
    "                index = j\n",
    "        c[index] = 1\n",
    "        generated_chars.append(index)\n",
    "\n",
    "    return generated_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " bVCb\n",
      ".rzp\n",
      "npexLMW:tFhPZMmgpPeOi:JMfMlFZGqAXm&3fbDG,A'bCaLQ,sLu3DKCVy-fyRPkZYkhziHQq\n",
      "DCbW Xsj,XmrAb;?UkMXyP-BrrrRUFsrCMSgixJ\n",
      "ye,LNaH';Dn$q,?pUMAlEC; ,QxViLhwaYD-$F.KX?'CAnB E;AZjdeAlf&coBrDB!LCF!K'FYuV \n",
      "----\n",
      "iter 0, loss: 534.321692\n",
      "----\n",
      " \n",
      "On'tmy mali? hins tras mot hi,\n",
      "As tou tous ad ont, artd seelepr to' lill aeaolhulbend do fin\n",
      "Buses thar hase bind, mound ilfos be tlun fom, seeas- soie wir to the at Sircy Fo\n",
      "Se' kt do aul-', nour.\n",
      "T \n",
      "----\n",
      "iter 1000, loss: 386.832871\n",
      "----\n",
      " Bum sorsa?\n",
      "\n",
      "GLuR:\n",
      "Herweepy mor dithe,\n",
      "Whi te by im Tow flt-itorslerac!\n",
      "\n",
      "blltif thooes doie nrof ar, whae deoitemese; am, fi nos aldy Tath cle ilsint lo\n",
      "Ad,\n",
      "Eyouliwe hyaus honnaakuont srans,\n",
      "I doul'coa \n",
      "----\n",
      "iter 2000, loss: 329.747959\n",
      "----\n",
      " Oveos, ranh?\n",
      "Whagd I swomkiseence'd ar.\n",
      "And\n",
      "Pacl ode.\n",
      "Ber bind'de of ofler not to tonpund;\n",
      "I\n",
      "ILY:\n",
      "Spener sees, melus fre angise tou seall, syere be iy lumes hus tis, dowe mer;\n",
      "Thangakm lol\n",
      "Cave her,\n",
      "B \n",
      "----\n",
      "iter 3000, loss: 308.595106\n",
      "----\n",
      " r-ho catst,\n",
      "Ill arelle:\n",
      "y as aotlit ighty thions bate wat.\n",
      "To hy.\n",
      "\n",
      "Oltf mrin:\n",
      "Whoom; jaus Takk not.\n",
      "\n",
      "Bur sei yim bo namlim ass'sath, dan touner, the thenk dome, sereavy.\n",
      "\n",
      "OLIPOCHOMIMEBETEMED:\n",
      "Tart aml \n",
      "----\n",
      "iter 4000, loss: 299.351715\n",
      "----\n",
      " d miaths's yore forimeobees cmaut feng'd, ar I spifhomyd I ;al witinpot aoth:\n",
      "Pxele, soss:\n",
      "AnTNG ming- ary ang Seres on Mege pint mard'ds Ig, stooed heen wite\n",
      "Freptep it,\n",
      "Bion sodad, yosees waqu's wis \n",
      "----\n",
      "iter 5000, loss: 294.705591\n",
      "----\n",
      " ed at nooaly:\n",
      "To rwe\n",
      "Anto,\n",
      "Io bir whe broums\n",
      "HiMy booees ateakes coruus to bar, rly thad binint I ind, fhist'd nhe a the ir coa hra thave.\n",
      "\n",
      "LARINEBE:\n",
      "E kens\n",
      "To hle hoong ha\n",
      "To fonhat birt?\n",
      "Yous bomat  \n",
      "----\n",
      "iter 6000, loss: 293.702714\n",
      "----\n",
      " ntr ap me ir thisist veory vikeRI:\n",
      "Ilou ot ract, ufe irer daw vaMI I pay we we I whaveiint at al -canh onlive?\n",
      "IT\n",
      "EMLO:y, dinne war coagh thas thew At dole you ise. fonoif hiir the y llet th fre\n",
      "Hus w \n",
      "----\n",
      "iter 7000, loss: 293.284855\n",
      "----\n",
      " l fare bouns hunee?\n",
      "\n",
      "PUSDAO:\n",
      "Shoup hay\n",
      "Theus pers se he,\n",
      "do\n",
      "I\n",
      "Theeed fay' lint I mirpably.\n",
      "\n",
      "SPUCENTI:\n",
      "Ill whan of.\n",
      "\n",
      "I':\n",
      "Buft mente nindll Pevroung he thiirermtheent!\n",
      "A:\n",
      "Maod bels thadees tr by arell.\n",
      " \n",
      "----\n",
      "iter 8000, loss: 292.193067\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-426532c2d4af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# loss, gradients, hprev = lossFun(inputs, targets, hprev)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mdWex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.999\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-191a7c6a18cb>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(activations, clipping)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# for the H term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mdbh\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdh_pre_activation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mdWhh\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdh_pre_activation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;31m# we need this term for the recurrent connection (previous bptt step needs this)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mdh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdh_pre_activation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# option = sys.argv[1]  # train or gradcheck\n",
    "option = 'train'\n",
    "\n",
    "if option == 'train':\n",
    "\n",
    "    n, p = 0, 0\n",
    "    smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "    while True:\n",
    "        # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "        if p+seq_length+1 >= len(data) or n == 0:\n",
    "            hprev = np.zeros((hidden_size, 1))  # reset RNN memory\n",
    "            p = 0 # go from start of data\n",
    "        inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "        targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "        # sample from the model now and then\n",
    "        if n % 1000 == 0:\n",
    "            sample_ix = sample(hprev, inputs[0], 200)\n",
    "            txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "            print ('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "        # forward seq_length characters through the net and fetch gradient\n",
    "        # loss, gradients, hprev = lossFun(inputs, targets, hprev)\n",
    "        loss, activations, memory = forward(inputs, targets, hprev)\n",
    "        gradients = backward(activations)\n",
    "        dWex, dWxh, dWhh, dWhy, dbh, dby = gradients\n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "        if n % 1000 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "        # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([Wex, Wxh, Whh, Why, bh, by],\n",
    "                                      [dWex, dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                      [mWex, mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "        p += seq_length # move data pointer\n",
    "        n += 1 # iteration counter\n",
    "\n",
    "elif option == 'gradcheck':\n",
    "\n",
    "    p = 0\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    delta = 0.001\n",
    "\n",
    "    hprev = np.zeros((hidden_size,1))\n",
    "    memory = hprev\n",
    "\n",
    "    loss, activations, memory = forward(inputs, targets, hprev)\n",
    "    # for gradient-checking we don't clip the gradients\n",
    "    gradients = backward(activations, clipping=False)\n",
    "    dWex, dWxh, dWhh, dWhy, dbh, dby = gradients\n",
    "\n",
    "    for weight, grad, name in zip([Wex, Wxh, Whh, Why, bh, by],\n",
    "                                  [dWex, dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  ['Wex', 'Wxh', 'Whh', 'Why', 'bh', 'by']):\n",
    "\n",
    "        str_ = (\"Dimensions dont match between weight and gradient %s and %s.\" % (weight.shape, grad.shape))\n",
    "        assert (weight.shape == grad.shape), str_\n",
    "\n",
    "        print(name)\n",
    "        for i in range(weight.size):\n",
    "\n",
    "            # evaluate cost at [x + delta] and [x - delta]\n",
    "            w = weight.flat[i]\n",
    "            weight.flat[i] = w + delta\n",
    "            loss_positive, _, _ = forward(inputs, targets, hprev)\n",
    "            weight.flat[i] = w - delta\n",
    "            loss_negative, _, _ = forward(inputs, targets, hprev)\n",
    "            weight.flat[i] = w # reset old value for this parameter\n",
    "            # fetch both numerical and analytic gradient\n",
    "            grad_analytic = grad.flat[i]\n",
    "            grad_numerical = (loss_positive - loss_negative) / ( 2 * delta )\n",
    "            rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "\n",
    "            if rel_error > 0.01:\n",
    "                print ('WARNING %f, %f => %e ' % (grad_numerical, grad_analytic, rel_error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
